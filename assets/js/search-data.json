{
  
    
        "post0": {
            "title": "Francais Perdition",
            "content": "Le français en voie de perdition? . Johannes Siedersleben, Paris, juillet 2013 . Les jeunes français ne connaissent plus Molière, Flaubert ou Maupassant ; Corneille, pour eux, est un chanteur. Les adultes commettent des erreurs regrettables : Ils parlent de la décision qu’ils ont pris, pour s’en rappeler plus tard et quand ils viennent vous voir, c’est ensemble avec leur épouse. Pire encore, le français se voit infiltré par une langue ennemie, à savoir l’anglais : Sans le moindre besoin, on remplace des mots français par des substituts anglais difficiles à prononcer et ne disant pas très bien ce qu’ils veulent dire ; on emploie des structures de phrases qui sont en français aussi laides qu’elles peuvent être élégantes en anglais. Ou cela nous mène-t-il ? Qu’est-ce que sera le français dans cent ans ? Restera-t-il des gens prêts à parler français et capables de le faire ? Allons voir pourquoi le français est probablement moins menacé que cela ne semble. . L’anglais est en effet la lingua franca du monde entier. . Cela est un fait à ne pas nier. Il y a au moins quatre raisons : la propagation de l’anglais sur tous les continents, le gradient raide de l’apprentissage, son rôle en économie, politique et science et son patrimoine littéraire. Si le français est égal ou même supérieur sur les deux derniers points, il ne l’est certainement pas sur les deux premiers : Par rapport à l’anglais, il est peu représenté en Amérique et en Asie, et il a de quoi intimider le débutant. Ça ne veut pas dire que l’anglais soit plus facile, mais l’expérience montre qu’il permet à l’apprenant d’atteindre un certain niveau avec peu d’effort, alors que le néophyte du français passe un temps considérable et décourageant à patauger dans les difficultés de la prononciation, de la grammaire et de l’orthographe. Le français est rébarbatif d’abord et accueillant ensuite; l’anglais feint de la simplicité et dupe ainsi le débutant. Mais le choix de la lingua franca est fait : Ce n’est ni le français, ni l’allemand, ni l’espagnol, ni le russe, ni l’esperanto, mais l’anglais. Il ne peut y avoir qu’un seul vainqueur. . L’anglais est-il à envier ? . Non, et ceci pour plusieurs raisons. L’anglais est menacé, lui aussi, justement et paradoxalement par le nombre immense de gens qui l’utilisent. Si le français paraît miné, c’est à cause de l’usage inapproprié ou négligent qu’on en fait : on le maîtrise mal ; mail, Twitter et autres outils de communication nous entraînent à des abréviations et des distorsions ou alors on s’en fiche simplement. L’ampleur de cet effet pour une langue donnée dépend du nombre d’utilisateurs, de leur hétérogénéité et de leur maîtrise de la langue : Plus il y a d’utilisateurs qui la parlent mal, plus ils ont tendance à l’altérer et diluer. C’est exactement ce qui arrive à l’anglais : L’anglais parlé dans un bazar indien ou dans une réunion de mathématiciens italiens, allemands et japonais est une offense infligée à l’anglais et on devrait féliciter le français d’être moins exposé à de pareils supplices. Les anglophones natifs sont tellement habitués au mauvais anglais qu’ils ont l’air de ne s’en même plus apercevoir. Vu de près, il y a à côté d’autres variétés deux anglais qu’Il faut bien distinguer: l’un réduit en guise d’esperanto comme moyen primitif de communication, et l’autre, l’anglais véritable, avec son vocabulaire immense, une foule d’idiomes et de fines nuances. Et en ce qui concerne la conservation et la valorisation du patrimoine littéraire, il est menacé autant que n’importe quelle autre langue : Le nombre de jeunes anglophones connaissant Shakespeare, Scott or Wilde est aussi peu élevé que le nombre de francophones érudits en littérature française. Donc, l’anglais tout entier n’est certainement p as menacé au point de vue propagation et il n’est pas non plus infiltré par d’autres langues. Mais sa pureté est en danger et la valorisation de sa littérature demande des efforts auprès des enseignants. . Quelle attitude prendre par rapport à l’anglais ? . Prenons au hasard des pays comme le Danemark, la Norvège ou la Finlande comme exemple. Ils ont, eux aussi, leur propre langue qui est un élément essentiel de leur identité nationale et dont ils sont fiers malgré un patrimoine littéraire peut-être moins impressionnant que celui de la France. Un pays chérit sa langue comme une mère son enfant : Elle le considère comme le plus mignon, le plus intelligent et le plus gentil du monde tout en sachant au fond de son âme que ce n’est peut-être pas toutà-fait vrai. . Quelle est l’attitude de ces pays par rapport à l’anglais ? Il y a au moins deux points qui sautent aux yeux : D’abord, ils sont complètement détendus. Ils se soucient peu des quelques anglicismes qui ont pu s’installer dans leur langue et sont convaincus qu’elle sera assez forte pour assimiler ces intrus. Ils ne sont pas craintifs mais fiers. Ensuite, ils maîtrisent l’anglais. A Oslo, n’importe quel passant vous indiquera le chemin en un anglais clair et correct. Ceci n’est pas seulement une question d’enseignement mais surtout un état d’esprit. Il va de soi que l’anglais joue aussi un rôle important dans leur enseignement supérieur : De nombreuses universités offrent au niveau master des programmes complètement en anglais et ne demandent aux candidats aucune connaissance préalable de la langue nationale. Ils attirent ainsi un grand nombre d’enseignants et d’étudiants qui, tout en faisant leur travail, sont involontairement amenés à un apprentissage linguistique qu’ils n’auraient jamais envisagé autrement. C’est donc paradoxalement grâce à l’ouverture à l’anglais que leur langue nationale aura gagné plus d’adeptes. Il va de soi que ce procédé ne se prête pas à tous les domaines de la même manière: Pour un cours de maths, la langue utilisée importe beaucoup moins que pour un cours de philosophie, et un cours de littérature norvégienne se fera nécessairement en norvégien. . Ce qui marche bien dans un pays ne marche pas forcément aussi bien dans un autre. Mais tout en évitant d’imiter candidement ce que font les autres, il pourrait être utile aux français de mieux apprendre l’anglais et de faciliter son utilisation dans l’enseignement supérieur. Vu de l’extérieur, il semble absurde qu’il faille une loi pour autoriser une grande école à donner des cours en anglais. Le choix de la langue employée devrait se faire naturellement en fonction des langues parlées par les personnes présentes comme c’est le cas lors d’une réunion de travail. Soit dit en passant il serait donc aberrant pour un professeur francophone d’enseigner des étudiants francophones en une langue autre que le français. . Les petits pays comme le Danemark, la Norvège ou la Finlande se font peu de soucis. Ils savent que leurs langues évoluent, que d’ici vingt ans, il y aura probablement d’avantage d’anglicismes et quelques règles de grammaire modifiées ou abolies. Mais nul ne met en doute le fait qu’on parlera le norvégien toujours à l’époque de nos arrière-petits-fils. Et si c’est vrai pour le norvégien, ce l’est à plus forte raison pour le français. Celui-ci occupera toujours une place importante au premier rang des grandes langues telles que l’allemand, l’espagnol, le russe ou l’anglais véritable, mais il n’est ni universel ni à priori supérieur à d’autres. .",
            "url": "https://johsieders.github.io/testfast/2022/01/04/francais-perdition.html",
            "relUrl": "/2022/01/04/francais-perdition.html",
            "date": " • Jan 4, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Baviere",
            "content": "La Bavière . Johannes Siedersleben, Paris, juillet 2013 . Qu’est-ce que la Bavière ? Mille choses à la fois. Non pas une montagne mais tout une palette allant de la montagne à vaches jusqu’aux murs vertigineux des Alpes. Non pas un lac mais une multitude de lacs rivalisant de beauté et de grâce. Non pas une rivière mais toute une famille de rivières qui se joignent les unes aux autres pour atteindre finalement soit la Mer Noire par le Danube, soit la Mer du Nord par le Main et le Rhin. Non pas un dialecte mais de nombreuses variations, chacune présentant des intonations inattendues, parfois drôles, et son propre lexique de gros mots. . Voyager en Bavière, c’est trouver le monde élégant à Munich ; rencontrer le Moyen Age à Nuremberg, Donauwörth ou Rothenburg ; s’immerger dans le silence et la solitude de la Forêt Bavaroise ; escalader les sommets des Alpes, les uns simples et accueillants, les autres difficiles et rébarbatifs; visiter les auberges dont beaucoup ont gardé leur charme d’autrefois et où l’on mange à la bonne franquette des plat abondants, savoureux et pas chers ; enfin, plaisir suprême par une journée chaude d’été, boire de la bière fraîche à l’ombre du châtaigner d’un Biergarten. . Voyager en Bavière, c’est aussi trouver des autoroutes, certes utiles, mais qui n’embellissent pas toujours le paysage ; s’étonner des rectifications de rivières comme l’Altmühl ou le Danube dont personne n’a jamais compris le sens ; rester perplexe devant quelques atrocités architecturales qui gâchent pas mal de villes et de paysages. . Alors, qu’est-ce que ce pays ? . La Bavière est un creuset ou se sont rencontrés peuples, cultures, artistes et armées. Comme tous les pays, elle a dû digérer des éléments de toutes sortes au cours de son histoire. Au Moyen Âge, la Bavière se bat contre les Autrichiens. Plus tard, elle accueille René Descartes qui a eu quelques-unes de ses meilleures idées dans un four bavarois (les fours étaient spacieux à cette époque). Pendant la guerre de Trente Ans, elle se bat aux côtés des Autrichiens contre les pays protestants et les Suédois sous Gustav-Adolf. Pendant l’ère napoléonienne, elle se bat d’abord contre la France, devient ensuite de fait une province française au sein de la confédération germanique et suit Napoléon bon gré mal gré jusqu’en Russie, laissant beaucoup de jeunes Bavarois perdus dans l’immensité de ce pays. Elle se bat encore aux côtés des Autrichiens contre la Prusse pour être intégrée enfin dans le Deutsche Reich. De nombreux artistes originaires de France ou d’Italie contribuent aux arts en général et à l’architecture en particulier. Qu’est-ce que serait Munich sans le Théâtre de Cuvilliés ou le château de Nymphenburg créé par des architectes italiens? Il y a des villes où l’on se croirait en Italie : la vieille ville de Rosenheim et le quartier riverain de Wasserburg sur Inn en sont des exemples. . La Bavière est un creuset de religions et d’idéologies. Son catholicisme toujours prédominant s’est donné, à quelques exceptions près bien sûr, une forme sympathique et tolérante. Là-bas, il est devenu une religion qui ne se prend pas trop au sérieux, qui est tolérante, peu dogmatique et qui sert surtout de prétexte à de nombreuses fêtes se terminant parfois en débauche. Aujourd’hui, catholiques, protestants, athées, musulmans et adhérents à bien d’autres religions vivent, tout compte fait, paisiblement ensemble. . La Bavière est une série de tableaux formant un tout à la fois hétéroclite et harmonieux : les paysans travaillent dans les champs avec le matériel le plus moderne ; les employés passent devant des façades historiques pour se rendre à leurs bureaux dans des tours d’acier et de verre ; les charpentiers mettent la poutre de faîte sur le toit ; les lacs sont parsemés de voiles blanches et les sommets de la haute montagne s’élancent vers le ciel ; les grimpeurs se désaltèrent devant un refuge en regardant avec respect le mur qu’ils se sont proposé d’escalader. La Bavière n’est pas un pays extraordinaire. Des montagnes, des lacs, des rivières, des villes et des villages, il y en a partout. Elle n’a rien qu’on ne trouve ailleurs. Et pourtant, elle est unique en réunissant tous ces éléments en un ensemble qui renforce les beautés et estompe les laideurs. .",
            "url": "https://johsieders.github.io/testfast/2022/01/03/baviere.html",
            "relUrl": "/2022/01/03/baviere.html",
            "date": " • Jan 3, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Animaux",
            "content": "Modeste Proposition . Johannes Siedersleben, Tuntenhausen, juillet 2020 . La France compte 65 millions d’habitants, 25 millions de porcs et 20 millions de vaches. Elle produit chaque année 240 millions d’hectolitres de lait et 2 millions de tonnes de volaille. La consommation de viande en France est de 90 kg par an et tête, bébés inclus. Étant donné ces chiffres, il n’est pas surprenant que ces bêtes soient élevées dans des conditions souvent abominables : quatre poulets par mètre carré, des veaux enfermés dans des cages qui les empêchent de bouger. Et les chiffres de tous les pays industrialisés se ressemblent. Nous parlons ici d’un problème mondial ; quel en est l’origine ? Bien sûr, on peut nommer l’inhumanité, la cruauté des éleveurs et aussi le manque de pitié, l’indifférence des consommateurs. Mais des diatribes contre les éleveurs et des appels aux consommateurs sont vite oubliés ; des règlements (tant de mètres carrés, des cages de tel gabarit) ne sont certes pas inutiles, mais ne résolvent pas le problème. Et si on convertissait les Français au végétarisme ou au véganisme ? Autant leur interdire le vin, et franchement, je ne sais pas si ça serait encore la France telle que nous l’aimons. . L’origine du mal, c’est notre gourmandise. Nous sommes insatiables. Personne n’a besoin de 90 kg de viande par an ; il est même malsain d’en manger tant. Mais la demande est là, et les éleveurs font l’impossible pour la satisfaire : Ils produisent le maximum aux plus bas coûts au détriment des animaux, des intérimaires dans les fermes et les abattoirs, et aussi de l’environnement : Dans les champs, on ne voit plus que du maïs ; chaque vache produit autant de CO2 par an qu’une voiture moyenne. Nous apercevons ici le côté sombre du capitalisme, qui n’est pas sans mérites mais défaillant en agriculture, dans les transports publics, les hôpitaux et bien d’autres domaines. . Voici ma modeste proposition : Mangeons moins de viande, la moitié par exemple, mais deux fois meilleure et deux fois plus chère. Les bêtes n’auraient plus d’antibiotiques, mais quatre fois plus de place. On ne mangerait de la viande que trois fois par semaine avec deux fois plus de plaisir. Les éleveurs gagneraient autant qu’avant, les consommateurs ne payeraient pas plus cher et les bêtes auraient la belle vie, un peu comme les moutons normands qui broutent l’herbe en profitant de la vue sur le Mont St. Michel. Une utopie ? Et si on commençait par une réduction de 10 pourcents au lieu de 50 ? .",
            "url": "https://johsieders.github.io/testfast/2022/01/02/animaux.html",
            "relUrl": "/2022/01/02/animaux.html",
            "date": " • Jan 2, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Bibliography",
            "content": "Heading IDs . How to Start . Bertand Russell: History of Western Philosophy Routledge, 1996, first published 1946 . This is one of the most popular introductions to philosophy and won Russell the Nobel Prize for Literature in 1950. It covers in roughly equal parts Antiquity, Middle Ages and Modernity. Plato and Aristotle are particularly well presented. Don’t read the German edition; the translation is awful. . Hans Joachim Störig: Kleine Weltgeschichte der Philosophie Kohlhammer Verlag, 2016 (18. Auflage), erste Auflage 1950 . Dies ist ein hervorragender Einstieg in die Philosophie . Antiquity . Pierre Hadot: Qu’est-ce que la philosophie antique? . Éditions Gallimard, 1995 . Karl Popper: The Open Society and its Enemies (Vol. 1): The Spell of Plato . Routledge, 2003, first published 1945 . Middle Ages . Peter Adamson: Medieval Philosophy . Oxford University Press, 2019 . Modern Subjects . Gaspard Koenig: Voyages d’un philosophe aux pays des libertés . Éditions de l’Observatoire, 2018 . Gaspard Koenig: La fin de l’individu . Éditions de l’Observatoire, 2019 . Gaspard Koenig: Notre vagabonde liberté . Éditions de l’Observatoire, 2021&gt; .",
            "url": "https://johsieders.github.io/testfast/2022/01/01/bibliography.html",
            "relUrl": "/2022/01/01/bibliography.html",
            "date": " • Jan 1, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Title",
            "content": "# QAware GmbH, Munich # 14.01.2022 . Entropy and Cross Entropy . This tutorial has two purposes: . It recaps the ideas of entropy and cross entropy. | It explains how they are implemented in Pytorch. | . Pytorch is often elegant and fast but not always straightforward. Our aim is to disentangle important functions such as CrossEntropyLoss BCELoss (binary cross entropy loss) BCELossWithLogits NLLLoss (non-negative log loss) softmax log_softmax sigmoid and logit. We explain for each of these: . input (format and meaning), | output (format and meaning), | what they exactly do, | and what the names mean (NLLLoss?). | . We are not interested in the plethora of optional parameters. The story is trivial for the initiated but possibly helpful for the rest of us. . import torch import torch.nn as nn from torch import arange, tensor, softmax, log_softmax, Tensor import matplotlib.pyplot as plt def show_diff(a: Tensor, b: Tensor): print(a.item() - b.item()) def plot(x, y, label, xlabel, ylabel): plt.plot(x, y, label=label) plt.legend(loc=&#39;center left&#39;, bbox_to_anchor=(1, 0.5)) plt.xlabel(xlabel) plt.ylabel(ylabel) . Entropy (Recap) . Consider a lottery with one number out of a million winning, all outcomes being equally likely. The information that, say, the number $n=123.456$ is not going to win is unsurprising while the converse, $n$ is guaranteed to win, would be of utmost interest. The lower the probability of an event, the higher the surprise. The surprise of an event is defined as the negative $ log$ of its probability: the smaller the probability, the higher the surprise. The surprise can also be thought of as the amount of information conveyed, it is in fact the number of bits needed to encode that information. . The entropy $H$ of a discrete probability distribution $ textbf{p} = [p_1, dots, p_n]$ is defined as the expected surprise or the expected amount of information: . $H( textbf{p}) = E[ log( textbf{p})] = - sum_{i=0}^{n-1} p_i log(p_i)$ . where it is understood that all probabilities are positive (just discard the zeros). The simplest case is the Bernoulli distribution $B(p)$ which describes the throw of a coin: . $P[X=1]=p$, $P[X=0]=1-p$. . We have . $H(p) = -[p log(p) + (1-p) log(1-p)]$ . The entropy&#39;s derivative . $ frac{d}{dp} H(p) = log(p) - log(1-p)$ . is zero at $p = 0.5$. The maximal entropy $log(2) approx 0.6931$ is located at $p = 0.5$, with minimum $0$ as $p$ approaches $0$ or $1$. This is a general principle: many equally likely events increase the entropy, few outstanding events decrease it: entropy measures disorder or lack of structure. For the discrete case, it analogously holds that . $0 lt H( textbf{p}) le log(n)$ . with the maximum $ log(n)$ at $ textbf{p} = [ frac{1}{n}, dots, frac{1}{n}]$ and the minimum $0$ at probabilities concentrated in one spot (a so called one-hot probability). . The entropy $H$ of a continuous probability density $f$ is defined as: . $H(f) = int_{ operatorname{supp} (f)} f(t) log(f(t))dt$, . The entropy of the uniform distribution on $[0, T]$ is simply $ log(T)$. Note that in the continuous case, the entropy can be $0$ (for $T = 1$) or negative (for $T in (0, 1)$). The entropy of the normal distribution $ mathcal{N}( mu, sigma)$ is $ frac {1}{2}(1+ log(2 sigma ^{2} pi))$, independent of $ mu$. . Entropy (Pytorch) . Pytorch feature numerous distributions; they all come with their entropy function. . from torch.distributions.bernoulli import Bernoulli x = arange(0.01, 1, 0.01) y = tensor([Bernoulli(p).entropy() for p in x]) plot(x, y, &#39;Bernoulli&#39; , &#39;p&#39;, &#39;entropy&#39;) . from torch.distributions.normal import Normal x = arange(0.01, 1, 0.01) y = tensor([Normal(tensor([0.]), tensor([s])).entropy() for s in x]) plot(x, y, &#39;Normal&#39; , &#39;s&#39;, &#39;entropy&#39;) . Some useful functions . We introduce $ operatorname{odds}$, $ operatorname{logit}$ and $ operatorname{softmax}$. . &quot;The odds are 5&quot; is a way of saying that winning is 5 times more likely than losing, which means that $p/(1 - p) = 5$, or $p = 5/6$. The definition is: . $ operatorname{odds}(p) = frac{p}{1-p}$. . $ operatorname{logit}$ is the $ log$ of odds: . $ operatorname{logit}(p) = log( frac{p}{1-p})$, . and $ operatorname{sigmoid}$ is the inverse of $ operatorname{logit}$: . $ operatorname{sigmoid}(x) = frac{1}{1 + exp(-x)}$, . which is thus confirmed: . x = tensor(2.) p = tensor(0.1) # logit undoes sigmoid show_diff(torch.logit(torch.sigmoid(x)), x) # sigmoid undoes logit show_diff(torch.sigmoid(torch.logit(p)), p) . -4.76837158203125e-07 0.0 . $ operatorname{sigmoid}$ normalizes any real number to the open interval $(0, 1)$. It is frequently used as activation function in neuronal networks. . $ operatorname{softmax}$ does for vectors what $ operatorname{sigmoid}$ does for scalars. It normalizes any vector $ textbf{x} = [x_1, dots, x_n]$ of real numbers (e.g. the output of a neuronal network) to a probability distribution: . $ operatorname{softmax}( textbf{x}) = left[ frac{ exp(x_i)}{ sum_{j=1}^{n}exp(x_j)} right]_{i=1, dots, n}$ . $ operatorname{softmax}$ is often combined with $ log$ for better numerical stability. $ operatorname{softmax}$ is only one of many possible normalizations. So, for instance, it would be natural to normalize $[ operatorname{logit}(p), operatorname{logit}(1-p)]$ to $[p, 1-p]$, but here is what $ operatorname{softmax}$ does: . $ operatorname{softmax}([ operatorname{logit}(p), operatorname{logit}(1-p)]) = left[ frac{p^2}{p^2+(1-p)^2}, frac{(1-p)^2}{p^2+(1-p)^2} right]$, . Cross Entropy (Recap) . The cross entropy of two discrete probability distributions $ textbf{p}$ and $ textbf{q}$ is defined as . $H( textbf{p}, textbf{q}) = -E_p [ log( textbf{q})] = - sum_{i=1}^{n-1} p_i log(q_i)$ . The cross entropy tells us how much information is contained in $ textbf{q}$ given $ textbf{p}$. It can be thought of as the gap between $ textbf{p}$ and $ textbf{q}$: the more additional information $ textbf{q}$ carries, the larger the gap. In more precise terms: . $H(p, q)$ is the expected extra message-length per datum that must be communicated if a code that is optimal for a given (wrong) distribution $q$ is used, compared to using a code based on the true distribution $p$:it is the excess entropy(see Wikipedia for details). . It is not symmetric, and we have $H( textbf{p}, textbf{p}) = H( textbf{p})$ which is never zero in the discrete case. . The business case of cross entropy is as follows: A known distribution $ textbf{p}$ (the one before the $ log$) is considered the truth or best guess. A second distribution $ textbf{q}$ (the one inside the $ log$) is assumed to be Bernoulli, Normal or whatever. The problem is to minimize the gap $H( textbf{p}, textbf{q})$ between $ textbf{p}$ and $ textbf{q}$ by choosing optimal parameters of the assumed distribution. In neuronal networks, cross entropy is used exactly as any other distance such as the mean square error (MSE). . Cross Entropy (Pytorch) . There are two Pytorch functions which do the job: CrossEntropyLoss and BCELoss (binary cross entropy loss) for the binary case. . Let us look at CrossEntropyLoss first. Up to some details, it takes two arrays of distributions (two matrices with shape (m, K)) and computes the mean cross entropy of all pairs. To understand the details, we proceed in three steps: . We define a function CELoss1(Q: Tensor, P: Tensor) -&gt; Tensor which accepts two matrices Q and P, each row being a probability distribution. The result is the mean of cross entropies H(Q(i), P(i)) (a tensor with one float element). Note that the order of arguments has changed with respect to $H(p, q)$: now the reference distribution P comes last. The shape of both P and Q is (m, K): m is the number of observations (or batch size), K the number of different classes; in the Bernoulli case, we have K = 2. . | We define a function CELoss2(Q: Tensor, y: Tensor) -&gt; Tensor, in which the second argument P is replaced with a one-hot representation: all but one p[i] are zero, and exactly one, say p[k] equals one, which is expressed by y[i] = k. So, the vector y replaces a huge spare matrix with exactly one non-zero element in each row (this is called the incidence matrix). . | We define a function CELoss3(X: Tensor, y: Tensor) -&gt; Tensor which differs from cross_entropy_onehot in that each row of the first argument X is transformed into a probability distribution via softmax. So, CELoss3 accepts any K-dimensional input. Up to some additional parameters does this function the same as Pytorch&#39;s CELoss. . | def CELoss1(Q: Tensor, P: Tensor) -&gt; Tensor: &quot;&quot;&quot; @param Q: matrix of estimated probability densities to be compared to P, shape = (m, K) @param P: matrix of reference probability densities, shape = (m, K) @return: -mean of row wise cross entropy of Q with respect to P &quot;&quot;&quot; # log(Q) has shape (m, K); log is elementwise applied # log(Q) * P has shape (m, K); this is the elementwise product # torch.sum(torch.log(Q) * P, 1) yields the vector of row sums return -torch.mean(torch.sum(torch.log(Q) * P, 1)) def CELoss2(Q: Tensor, y: Tensor) -&gt; Tensor: &quot;&quot;&quot; @param Q: matrix of estimated probability densities to be compared to y, shape = (m, K) @param y: vector of true labels; 0 &lt;= y[i] &lt; K = Q.shape[1], shape = (m,) y represents a one-hot probability @return: mean of row wise cross entropy of Q with respect to p &quot;&quot;&quot; x = range(Q.shape[0]) # m # Q[x, y] is the same as [Q[i, y[i]] for i in range(m)] return -torch.mean(torch.log(Q[x, y])) def CELoss3(X: Tensor, y: Tensor) -&gt; Tensor: &quot;&quot;&quot; @param X: any vector of shape (n,) X is transformed into a probability density Q via softmax @param y: the true label; 0 &lt;= y &lt; K = q.shape y represents the one-hot probability @return: cross entropy of q with respect to y &quot;&quot;&quot; return CELoss2(softmax(X, 1), y) CELoss = nn.CrossEntropyLoss() # CELoss and CELoss3 yield almost the same results. X = tensor([[1., 1., 2., 3.], [3., 4., 1., 7.]]) y = tensor([3, 3]) show_diff(CELoss(X[[1]], y[[1]]), CELoss3(X[[1]], y[[1]])) show_diff(CELoss(X, y), CELoss3(X, y)) . -2.9802322387695312e-08 -2.9802322387695312e-08 . A Numerical Issue . In CELoss3 we first compute softmax with lots of exponents and then take the log in CELoss2. It is convenient to shift the log from CELoss2 to CELoss3. So, CELoss2 becomes NLLLoss1, equivalent to NLLLoss, and CELoss3 becomes CELoss4, equivalent to CrossEntropyLoss. NLLLoss is so called because it expects its input to be just that, a non-negative log loss. . def NLLLoss1(Q: Tensor, y: Tensor) -&gt; Tensor: &quot;&quot;&quot; Negative Loss Likelihood Loss @param Q: matrix of estimated probability densities to be compared to y, shape = (m, K) @param y: the true label; 0 &lt;= y &lt; K = q.shape @return: -mean of [Q[i, y[i]] for i in range(m)] &quot;&quot;&quot; x = range(Q.shape[0]) # m # Q[x, y] is the same as [Q[i, y[i]] for i in range(m)] return -torch.mean(Q[x, y]) def CELoss4(X: Tensor, y: Tensor) -&gt; Tensor: &quot;&quot;&quot; This is identical to CELoss3 but numerically more stable @param X: any vector of shape (n,) X is transformed into a probability density Q via softmax @param y: the true label; 0 &lt;= y &lt; K = q.shape This represents the one-hot probability p concentrated at y @return: cross entropy of q with respect to y &quot;&quot;&quot; return NLLLoss1(log_softmax(X, 1), y) # CELoss and CELoss4 yield exactly the same results X = tensor([[1., 1., 2., 3.], [3., 4., 1., 7.]]) y = tensor([3, 3]) show_diff(CELoss(X[[1]], y[[1]]), CELoss4(X[[1]], y[[1]])) show_diff(CELoss(X, y), CELoss4(X, y)) . 0.0 0.0 . Cross Entropy Loss and Binary Cross Entropy (Pytorch) . As the rows of P, the first argument of CELoss, sum up to 1, we could dispose of one column. This is what BCELoss does in the binary case. It serves as a shorthand for CrossEntropyLoss. Its input are two tensors of shape (m,), the first one, P, with probabilities, the second one, y with binary labels. The missing second column of P is computed as 1 - P. See BCELoss1 for an implementation. . BCELoss is not quite consistent with CrossEntropyLoss. Comparing BCELoss with CELossis tricky because the latter applies softmax while the former uses sigmoid. We proceed as follows: . The input to CELoss is some tensor X and binary labels y both with shape (m, 2). The input to BCELoss is obtained by taking the second column of softmax(X). BCELoss requires the binary labels to be float. . def BCELoss1(q: Tensor, y: Tensor) -&gt; Tensor: ones = torch.ones_like(y) return -torch.mean(y * torch.log(q) + (ones - y) * torch.log(ones - q)) CELoss = nn.CrossEntropyLoss() BCELoss = nn.BCELoss() # Input for CELoss: # X.shape = (m, 2), because it&#39;s binary X = tensor([[8., 2.], [3., 4.], [8., 3.]]) # y contains only 0 and 1 y = tensor([0, 1, 0]) # Input for BCELoss (and BCELoss1): # softmax(X, dim=1) contains in each row a binary probability density # Q is the second column of softmax(X, dim=1) Q = softmax(X, dim=1)[:, 1] y1 = y.to(torch.float) # doesn&#39;t make sense # CELoss and BCELoss are almost equal, show_diff(CELoss(X, y), BCELoss(Q, y1)) # while BCELoss and BCELoss1 completely agree. show_diff(BCELoss(Q, y1), BCELoss1(Q, y)) . -7.450580596923828e-09 0.0 . BCELoss and BCEWithLogitsLoss (Pytorch) . BCELoss comes in two varieties: The first one, BCELoss accepts as first argument a vector P of probabilities. This corresponds to CELoss1. The second one, BCEWithLogitsLoss, accepts as first argument any real vector which is normalized by sigmoid rather than softmax as in CrossEntropyLoss. BCEWithLogitsLoss is so called because the first argument is expected to be a logit. . BCEWithLogitsLoss = nn.BCEWithLogitsLoss() logit = torch.logit Q = tensor([0.1, 0.3, 0.6]) y = tensor([1., 0., 1.]) show_diff(BCELoss(Q, y), BCEWithLogitsLoss(logit(Q), y)) . 0.0 .",
            "url": "https://johsieders.github.io/testfast/2021/01/02/crossentropy.html",
            "relUrl": "/2021/01/02/crossentropy.html",
            "date": " • Jan 2, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Title",
            "content": "# QAware GmbH, Munich # 14.01.2022 . What is the Problem? . A physician is to assess blood counts consisting of some 20 values. Each of them is labeled either healthy or pathological. The problem is to decide automatically for any given blood count whether it is healthy. The classical approach is to describe the decision by a set of rules, to program them and the problem is solved. If these rules are available there is no need for artificial intelligence (AI). But often they are hard or impossible to come by and this is when AI comes in. . Some terminology before carrying on: . AI is all about the feature matrix $X$ and the target vector $Y$. Each blood count is an item, represented by a row of $X$, each value of the blood count is a feature, represented by a column, and each label (healthy or not healthy) is an entry of $Y$. The labels can be binary or n-ary, when different diseases are to be identified. The number $m$ of rows can be huge (millions or more), the number $n$ of features tends to be moderate (tens or hundreds). . Other examples: . (1) We are given a set of mails labeled spam or no spam. Each mail is tokenized as a bag of words (BOW). The BOW of a mail (or whatever text) indicates for each word the number of occurrences in that mail. . (2) Passport photos are labelled maleor female. Each photo is represented as a $(n_0 times n_1)$-matrix, so the feature matrix becomes a cube of dimension $m times n_0 times n_1$. . The classification problem of AI goes as follows: We are given a two feature matrices $X_{train}$, $X_{test}$, and two target vectors $Y_{train}$, $Y_{test}$. The training set is the tuple $(X_{train}, Y_{train})$, the test set is $(X_{test}, Y_{test})$. . The problem is to find a function $F$ based only on the training set which maps any item $x$ to a label $y$ in such a way that $F$ performs well on the test set; ideally we have $F(x) = y$ for all $(x, y)$ in the test set. Such a function is called a classifier. . As mentioned before, the problem is almost trivial if we can find the rules which govern the classification. If not, we can try various AI approaches such as the perceptron, Bayesian methods, neuronal networks and others. . No matter how we found the classifier: Once we trust it, we can apply it in practice with two looming risks. One is obvious: It all depends on how well training and test sets represent reality. If they don&#39;t the classifier will perform poorly. The other one is ethical and well presented by Cathy O&#39;Neill: The Weapons of Math Destruction. O&#39;Neill gives striking examples of how AI can lead us astray. . A few remarks are in order. . The feature matrix can be a cube or a higher dimensional tensor, but it is always numeric. So, whatever the input, it must be transformed into a numeric format. The BOW (bag of words) is just one way of many to transform text into numbers. . | The features should be normalized. We are running into numerical troubles if the values of feature A are in the millions and those of feature B in small fractions. . | Features can be more or less significant: if a column is constant across all items it should be dropped. One of the challenges of classification is to identify the significant features and to get rid of the rest. . | Features can be dependent or independent. A famous example goes as follows: The items represent men, the features are baldness and age, the label is wealthy. Common sense tells us that age governs both wealth and baldness but how can an algorithm know? Getting rid of dependent features is not always easy; see J. Pearl: The Book of Why. The database people use the normal forms to tackle the same problem. . | The quality of a classifier with respect to a given set can be assessed by different metrics. These include: . accuracy = (true positives + true negatives) / m . precision = true positives / (true positives + false positives) precision is the share of true positives among all positives found. . recall = true positives / (true positives + false negatives) . recall is the share of found positives among all positives. . A prediction which returns always positive has recall $1$, . | import torch import torch.nn as nn import torch.optim as optim import pandas as pd from torch import tensor from collections.abc import Callable import matplotlib.pyplot as plt from plotting import plot_protocol, plot_confusion_matrix, plot_diagram from learner import Learner import perceptron # uncomment this on jupyterlab # import sys # sys.path.insert(0, &#39;../shared&#39;) # from metrics import plot_confusion_matrix #, plot_loss . Comparing Classifiers . The idea is to compare classifiers on data sets which ca be easily programmed, such as boolean functions (AND, OR an XOR), or geometrical shapes such as half-planes, circles, squares or concentric rings. So, train and test set can be easily generated; the performance of different methods becomes obvious. . Each of these functions returns 1 on a subset of $R^2$ (a half-plane, a circle or a square) and 0 on its complement. On concentric rings, the corresponding function alternates between 1 and 0. . So, p = plane([0, 1], 0) defines the lower half-plane in $R^2$; p(X) returns 1 iff $X &lt;= 0$. Likewise, c = circle(1) defines a circle with radius 1 around the origin; c(X) returns 1 if X is in the circle or on its boundary, else 0. . AND, OR and half-planes are linearly separable, XOR, circles and squares are not. . What are the classifiers we are going to test? You will find classifiers based on neuronal networks (NN-classifiers) and the perceptron. Feel free to add whatever classifier you would like to try. . The perceptron is the grandfather of all classifiers. It is known to work well on sets which are linearly separable and notoriously badly on all others. It is available in two varieties: the one provided by sklearn, and our own implementation. . Defining Classifiers . NN-classifiers are made of three ingredients: a module, an optimizer and a loss function. The module defines the network topology such as nn.Linear(2, 1), activation functions such as nn.Sigmoid() and it may include technical elements such as Tovector which transforms a (m, 1)-matrix into a (m,)-vector. PyTorch implements numerous optimizers; we have chosen the Adam optimizer but others might work equally well or better. Find out if you are interested. . The class Learner is where the optimizing happens. It affords two methods: fit and predict. fit minimizes the loss function by adjusting its internal weights. The training loop within the fit-method is the innermost kernel of neuronal networks. fit accepts the training set (a feature matrix and a target vector) and doesn&#39;t need to return anything; our Learner however returns a protocol which indicates how the loss evolved. The training loop is parametrised by the learning rate Lr, a small real value, and the number of epochs n_epoch, which indicates how often the input is to be processed. predict accepts the feature matrix from the test set and returns the raw prediction which is to be interpreted depending on the classifier used: rounding for MSE (mean square error loss), argmax for CE (cross entropy loss), heaviside for BCE (binary cross entropy Loss). We obtain the desired metric ( accuracy, precision, recall or others) by comparing the resulting prediction (after interpretation) with the test set&#39;s target vector (the truth). . A large feature matrix can be split into several minibatches; fit would be called on each of them. . The classifier regressor21 is bare linear regression, regressor21s, regressor251 are variants thereof. They all use MSE which is normally used to approximate continuous functions. Discretising the prediction by rounding is frowned upon by experts but can work remarkably well. . classifier21 and classifier22 are bare logistic regression, with BCE and CE as loss function respectively. The difference between BCE and CE is technical; find more information here. There remaining ones are variants of logistic regression: classifier22, classifier252, classifier2XX2. . class Tovector(nn.Module): &quot;&quot;&quot; Transforms a (m, 1)-matrix into a (m,)-vector &quot;&quot;&quot; def forward(self, X: tensor) -&gt; tensor: return X.view(-1) def regressor21(cuda: bool) -&gt; Learner: F = nn.Sequential(nn.Linear(2, 1), Tovector()) if cuda: F.cuda() return Learner(F, optim.Adam, nn.MSELoss) def regressor21s(cuda: bool) -&gt; Learner: F = nn.Sequential(nn.Linear(2, 1), Tovector(), nn.Sigmoid()) if cuda: F.cuda() return Learner(F, optim.Adam, nn.MSELoss) def regressor251(cuda: bool) -&gt; Learner: F = nn.Sequential(nn.Linear(2, 7), nn.Sigmoid(), nn.Linear(7, 1), Tovector()) if cuda: F.cuda() return Learner(F, optim.Adam, nn.MSELoss) def classifier21(cuda: bool) -&gt; Learner: F = nn.Sequential(nn.Linear(2, 1), Tovector()) if cuda: F.cuda() return Learner(F, optim.Adam, nn.BCEWithLogitsLoss) def classifier22(cuda: bool) -&gt; Learner: F = nn.Linear(2, 2) if cuda: F.cuda() return Learner(F, optim.Adam, nn.CrossEntropyLoss) def classifier251(cuda: bool) -&gt; Learner: F = nn.Sequential(nn.Linear(2, 7), nn.Sigmoid(), nn.Linear(7, 1), Tovector()) if cuda: F.cuda() return Learner(F, optim.Adam, nn.BCEWithLogitsLoss) def classifier252(cuda: bool) -&gt; Learner: F = nn.Sequential(nn.Linear(2, 7), nn.Sigmoid(), nn.Linear(7, 2)) if cuda: F.cuda() return Learner(F, optim.Adam, nn.CrossEntropyLoss) def classifier2XX2(cuda: bool) -&gt; Learner: F = nn.Sequential(nn.Linear(2, 8), nn.Linear(8, 3), nn.ReLU(), nn.Linear(3,2)) if cuda: F.cuda() return Learner(F, optim.Adagrad, nn.CrossEntropyLoss) from sklearn.linear_model import Perceptron def sk_perceptron() -&gt; Perceptron: return Perceptron() def my_perceptron() -&gt; perceptron.Perceptron: return perceptron.Perceptron() . Testing Classifiers . All test functions work along the same pattern: X is the feature matrix of the training set, X_t that of the test set. The functions are the decision functions to by applied, e.g. and_, plane, circle. The sequence of actions is to build a learner, to train it, to compute truth and prediction and to log everything. The raw prediction returned must be refined by rounding, heaviside or argmax. . def test_regressor(regressor, functions, lr, n_epochs, X, X_t, log, cuda): &quot;&quot;&quot; @param regressor: one of regressor21, regressor21s, regressor252 @param functions: a list of test functions such as and_, plane, circle @param lr: learning rate @param n_epochs: number of epochs @param X: the feature matrix of the training set @param X_t: the feature matrix of the test set @param log: the log contains the protocol, the truth Y_t and the prediction Y_pt @param cuda: true if cuda is to be used @return: None &quot;&quot;&quot; for f in functions: learner = regressor(cuda) # build a learner protocol = learner.fit(X, f(X), lr, n_epochs) # training Y_t = f(X_t) # save the truth Y_pt = torch.round(learner.predict(X_t)) # predict log[regressor, f] = protocol, Y_t.detach().cpu(), Y_pt.detach().cpu() # log def test_classifier_bce(classifier, functions, lr, n_epochs, X, X_t, log, cuda): &quot;&quot;&quot; @param classifier: one of classifier21, classifier251 @param functions: a list of test functions such as and_, plane, circle @param lr: learning rate @param n_epochs: number of epochs @param X: the feature matrix of the training set @param X_t: the feature matrix of the test set @param log: the log contains the protocol, the truth Y_t and the prediction Y_pt @param cuda: true if cuda is to be used @return: None &quot;&quot;&quot; for f in functions: learner = classifier(cuda) protocol = learner.fit(X, f(X), lr, n_epochs) Y_t = f(X_t) Y_pt = torch.heaviside(learner.predict(X_t), torch.zeros(1, device=X_t.device)) log[classifier, f] = protocol, Y_t.detach().cpu(), Y_pt.detach().cpu() def test_classifier_ce(classifier, functions, lr, n_epochs, X, X_t, log, cuda): &quot;&quot;&quot; @param classifier: one of classifier22, classifier252, classifier2XX2 @param functions: a list of test functions such as and_, plane, circle @param lr: learning rate @param n_epochs: number of epochs @param X: the feature matrix of the training set @param X_t: the feature matrix of the test set @param log: the log contains the protocol, the truth Y_t and the prediction Y_pt @param cuda: true if cuda is to be used @return: None &quot;&quot;&quot; for f in functions: Y = f(X).to(torch.long) learner = classifier(cuda) protocol = learner.fit(X, Y, lr, n_epochs) Y_t = f(X_t).to(torch.long) Y_pt = torch.argmax(learner.predict(X_t), dim=1) log[classifier, f] = protocol, Y_t.detach().cpu(), Y_pt.detach().cpu() def test_my_perceptron(functions, lr, n_epochs, X, X_t, log): &quot;&quot;&quot; @param classifier: one of classifier21, classifier22, classifier252 @param functions: a list of test functions such as and_, plane, circle @param lr: learning rate @param n_epochs: number of epochs @param X: the feature matrix of the training set @param X_t: the feature matrix of the test set @param log: protocol, the truth Y_t and the prediction Y_pt are appended to log @return: None &quot;&quot;&quot; learner = my_perceptron() for f in functions: Y_t = f(X_t) protocol = learner.fit(X, f(X), lr, n_epochs) Y_pt = learner.predict(X_t) log[my_perceptron, f] = protocol, Y_t.cpu(), Y_pt.cpu() def test_sk_perceptron(functions, X, X_t, log): &quot;&quot;&quot; @param classifier: one of classifier21, classifier22, classifier252 @param functions: a list of test functions such as and_, plane, circle @param X: the feature matrix of the training set @param X_t: the feature matrix of the test set @param log: the truth Y_t and the prediction Y_pt are appended to log; no protocol @return: None &quot;&quot;&quot; learner = sk_perceptron() X_loc = X.cpu() for f in functions: Y_t = f(X_t).cpu() Y = f(X).cpu() learner.fit(X_loc, Y) Y_pt = learner.predict(X_t.cpu()) log[sk_perceptron, f] = [], Y_t, torch.tensor(Y_pt) . Running the Tests . We unpack the configuration to get the decision functions to be used, the variants to be tested, and some parameters: learning rate, n_epochs and whether cuda is to be applied. We call all relevant variants and obtain a log which contains protocol, truth Y_t and prediction Y_pt for each tuple (variant, function). sk_perceptron provides no protocol. . Once all tests are run, we collect and print the errors (any metric can go here), we print confusion matrices and diagrams, which show the graph, all positive points in red (positive is bad), and all negative points green. False positives are represented as green crosses (they are negative), false negatives as red crosses (they are negative). Finally, we plot the protocols which show how the variant converge or fail to converge. This is done twice: in the first run we compare how a given variant performs on different decision functions, in the second, how different variants perform on a given decision function. . def run_test(cfg, cuda): # unpack configuration functions = cfg[&#39;functions&#39;] variants = cfg[&#39;variants&#39;] lr = cfg[&#39;lr&#39;] n_epochs = cfg[&#39;n_epochs&#39;] X = cfg[&#39;X&#39;] X_t = cfg[&#39;X_t&#39;] if cuda: X = X.cuda() X_t = X_t.cuda() log = {} # run selected tests if regressor21 in variants: test_regressor(regressor21, functions, lr, n_epochs, X, X_t, log, cuda) if regressor21s in variants: test_regressor(regressor21s, functions, lr, n_epochs, X, X_t, log, cuda) if regressor251 in variants: test_regressor(regressor251, functions, lr, n_epochs, X, X_t, log, cuda) if classifier21 in variants: test_classifier_bce(classifier21, functions, lr, n_epochs, X, X_t, log, cuda) if classifier251 in variants: test_classifier_bce(classifier251, functions, lr, n_epochs, X, X_t, log, cuda) if classifier22 in variants: test_classifier_ce(classifier22, functions, lr, n_epochs, X, X_t, log, cuda) if classifier252 in variants: test_classifier_ce(classifier252, functions, lr, n_epochs, X, X_t, log, cuda) if classifier2XX2 in variants: test_classifier_ce(classifier2XX2, functions, lr, n_epochs, X, X_t, log, cuda) if my_perceptron in variants: test_my_perceptron(functions, lr, n_epochs, X, X_t, log) if sk_perceptron in variants: test_sk_perceptron(functions, X, X_t, log) # collect errors errors = pd.DataFrame(index=[v.__name__ for v in variants]) for f in functions: column = [] for v in variants: _, Y_t, Y_pt = log[(v, f)] error = (torch.sum(torch.abs(Y_t - Y_pt)).item()) / Y_t.shape[0] column.append(error) errors[f.__name__] = column # print errors errors[&#39;avg&#39;] = errors.sum(axis=1) / len(functions) print(errors, &#39; n&#39;, &#39; n&#39;) # plot confusion matrices and diagrams for f in functions: for v in variants: _, Y_t, Y_pt = log[(v, f)] plot_confusion_matrix(Y_t, Y_pt, v.__name__ + &#39; - &#39; + f.__name__) if f not in [and_, or_, xor_]: plot_diagram(X_t.cpu(), Y_t, Y_pt, f) # plot protocols by functions for f in functions: plt.figure() for v in variants: protocol, Y_t, Y_pt = log[(v, f)] if len(protocol) &gt; 0: plot_protocol(protocol, f.__name__, v.__name__) # plot protocols by variants for v in variants: plt.figure() for f in functions: protocol, _, _ = log[(v, f)] if len(protocol) &gt; 0: plot_protocol(protocol, v.__name__, f.__name__) . Defining Configurations . Each configuration contains the feature matrices X for training and X_t for test (generated or given) the variants to be tested, the learning rate and the number of epochs. . config = {} config[&#39;A&#39;] = {&#39;variants&#39;: [my_perceptron, sk_perceptron], &#39;lr&#39;: 1e-2, &#39;n_epochs&#39;: 100, &#39;X&#39;: tensor([[0., 0.], [0., 1.], [1., 0.], [1., 1.]]), &#39;X_t&#39;: tensor([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])} config[&#39;B&#39;] = {&#39;variants&#39;: [sk_perceptron, my_perceptron, regressor21, regressor21s, regressor251, classifier21, classifier251, classifier22, classifier252, classifier2XX2], &#39;lr&#39;: 1e-2, &#39;n_epochs&#39;: 100, &#39;X&#39;: tensor([[0., 0.], [0., 1.], [1., 0.], [1., 1.]]), &#39;X_t&#39;: tensor([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])} config[&#39;C&#39;] = {&#39;variants&#39;: [regressor21, regressor21s, regressor251, classifier21, classifier251, classifier22, classifier252, classifier2XX2], &#39;lr&#39;: 1e-2, &#39;n_epochs&#39;: 100, &#39;X&#39;: torch.rand((100, 2)) * 10, &#39;X_t&#39;: torch.rand((100, 2)) * 10} config[&#39;D&#39;] = {&#39;variants&#39;: [regressor251, classifier251, classifier252], &#39;lr&#39;: 1e-2, &#39;n_epochs&#39;: 5000, &#39;X&#39;: torch.rand((500, 2)) * 10, &#39;X_t&#39;: torch.rand((100, 2)) * 10} config[&#39;P&#39;] = {&#39;variants&#39;: [sk_perceptron, my_perceptron], &#39;lr&#39;: 5e-2, &#39;n_epochs&#39;: 500, &#39;X&#39;: torch.rand((200, 2)) * 10, &#39;X_t&#39;: torch.rand((100, 2)) * 10} config[&#39;R&#39;] = {&#39;variants&#39;: [regressor251, classifier251, classifier252, classifier2XX2], &#39;lr&#39;: 1e-3, &#39;n_epochs&#39;: 15000, &#39;X&#39;: torch.rand((500, 2)) * 10, &#39;X_t&#39;: torch.rand((100, 2)) * 10} . Launch . We define some decision functions, complement the configurations and run one of them. . from classifier_fcts import and_, xor_, or_, Plane, Circle, Square, Rings cuda = True plane0 = Plane([-1, 1], 0, cuda) # -x0 + x1 = 0 plane1 = Plane([1, 1], -10, cuda) # x0 + x1 = 10 plane2 = Plane([0, 1], -5, cuda) # x1 = 5 circle8 = Circle(8) # radius = 8 square7 = Square(7) # side length = 7 rings3 = Rings(1) # width of ring = 3 config[&#39;A&#39;][&#39;functions&#39;] = [and_, or_, xor_] config[&#39;B&#39;][&#39;functions&#39;] = [and_, or_, xor_] config[&#39;C&#39;][&#39;functions&#39;] = [plane0, circle8, square7, rings3] config[&#39;D&#39;][&#39;functions&#39;] = [plane0, plane2, circle8, square7, rings3] config[&#39;P&#39;][&#39;functions&#39;] = [plane0, plane2, circle8, square7, rings3] config[&#39;R&#39;][&#39;functions&#39;] = [rings3] run_test(config[&#39;R&#39;], cuda) . Rings1 avg regressor251 0.47 0.47 classifier251 0.54 0.54 classifier252 0.53 0.53 classifier2XX2 0.48 0.48 .",
            "url": "https://johsieders.github.io/testfast/2021/01/01/classifier_lab.html",
            "relUrl": "/2021/01/01/classifier_lab.html",
            "date": " • Jan 1, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://johsieders.github.io/testfast/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://johsieders.github.io/testfast/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://johsieders.github.io/testfast/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://johsieders.github.io/testfast/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}