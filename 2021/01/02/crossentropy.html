<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Title | fastpages</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Title" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="An easy to use blogging platform with support for Jupyter Notebooks." />
<meta property="og:description" content="An easy to use blogging platform with support for Jupyter Notebooks." />
<link rel="canonical" href="https://johsieders.github.io/testfast/2021/01/02/crossentropy.html" />
<meta property="og:url" content="https://johsieders.github.io/testfast/2021/01/02/crossentropy.html" />
<meta property="og:site_name" content="fastpages" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-01-02T00:00:00-06:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Title" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2021-01-02T00:00:00-06:00","datePublished":"2021-01-02T00:00:00-06:00","description":"An easy to use blogging platform with support for Jupyter Notebooks.","headline":"Title","mainEntityOfPage":{"@type":"WebPage","@id":"https://johsieders.github.io/testfast/2021/01/02/crossentropy.html"},"url":"https://johsieders.github.io/testfast/2021/01/02/crossentropy.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/testfast/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://johsieders.github.io/testfast/feed.xml" title="fastpages" /><link rel="shortcut icon" type="image/x-icon" href="/testfast/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/testfast/">fastpages</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/testfast/about/">About Me</a><a class="page-link" href="/testfast/search/">Search</a><a class="page-link" href="/testfast/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Title</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-01-02T00:00:00-06:00" itemprop="datePublished">
        Jan 2, 2021
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      11 min read
    
</span></p>

    

    
      
        <div class="pb-5 d-flex flex-justify-center">
          <div class="px-2">

    <a href="https://github.com/johsieders/testfast/tree/master/_notebooks/2021-01-02-crossentropy.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/testfast/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/johsieders/testfast/master?filepath=_notebooks%2F2021-01-02-crossentropy.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/testfast/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/johsieders/testfast/blob/master/_notebooks/2021-01-02-crossentropy.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/testfast/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
          <div class="px-2">
  <a href="https://deepnote.com/launch?url=https%3A%2F%2Fgithub.com%2Fjohsieders%2Ftestfast%2Fblob%2Fmaster%2F_notebooks%2F2021-01-02-crossentropy.ipynb" target="_blank">
      <img class="notebook-badge-image" src="/testfast/assets/badges/deepnote.svg" alt="Launch in Deepnote"/>
  </a>
</div>

        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-01-02-crossentropy.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># QAware GmbH, Munich</span>
<span class="c1"># 14.01.2022</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Entropy-and-Cross-Entropy">Entropy and Cross Entropy<a class="anchor-link" href="#Entropy-and-Cross-Entropy"> </a></h2><p>This tutorial has two purposes:</p>
<ul>
<li>It recaps the ideas of entropy and cross entropy.</li>
<li>It explains how they are implemented in Pytorch.</li>
</ul>
<p>Pytorch is often elegant and fast but not always straightforward. Our aim is to disentangle important functions
such as
<code>CrossEntropyLoss</code>
<code>BCELoss</code> (binary cross entropy loss)
<code>BCELossWithLogits</code>
<code>NLLLoss</code> (non-negative log loss)
<code>softmax</code>
<code>log_softmax</code>
<code>sigmoid</code> and
<code>logit</code>.
We explain for each of these:</p>
<ul>
<li>input (format and meaning),</li>
<li>output (format and meaning),</li>
<li>what they exactly do,</li>
<li>and what the names mean (<code>NLLLoss</code>?).</li>
</ul>
<p>We are not interested in the plethora of optional parameters.
The story is trivial for the initiated but possibly helpful for the rest of us.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">arange</span><span class="p">,</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">softmax</span><span class="p">,</span> <span class="n">log_softmax</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">show_diff</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">-</span> <span class="n">b</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

<span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">xlabel</span><span class="p">,</span> <span class="n">ylabel</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;center left&#39;</span><span class="p">,</span> <span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">xlabel</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">ylabel</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Entropy-(Recap)">Entropy (Recap)<a class="anchor-link" href="#Entropy-(Recap)"> </a></h3><p>Consider a lottery with one number out of a million winning, all outcomes being equally likely.
The information that, say, the number $n=123.456$ is <strong>not</strong> going to win is unsurprising while the converse,
$n$ is guaranteed to win, would be of utmost interest. The lower the probability of an event,
the higher the surprise. The surprise of an event is defined as the negative $\log$ of its probability: the smaller the probability, the higher the surprise.
The surprise can also be thought of as the amount of information conveyed,
it is in fact the number of bits needed to encode that information.</p>
<p>The entropy $H$ of a discrete probability distribution $\textbf{p} = [p_1, \dots, p_n]$ is defined as the expected surprise or
the expected amount of information:</p>
<p>$H(\textbf{p}) = E[\log(\textbf{p})] = -\sum_{i=0}^{n-1} p_i \log(p_i)$</p>
<p>where it is understood that all probabilities are positive (just discard the zeros).
The simplest case is the Bernoulli distribution $B(p)$ which describes the throw of a coin:</p>
<p>$P[X=1]=p$, $P[X=0]=1-p$.</p>
<p>We have</p>
<p>$H(p) = -[p\log(p) + (1-p)\log(1-p)]$</p>
<p>The entropy's derivative</p>
<p>$\frac{d}{dp} H(p) = \log(p) - \log(1-p)$</p>
<p>is zero at $p = 0.5$. The maximal entropy $log(2) \approx 0.6931$  is located at $p = 0.5$, with
minimum $0$ as $p$ approaches $0$ or $1$. This is a general principle: many equally likely events
increase the entropy, few outstanding events decrease it: entropy measures disorder or lack of structure.
For the discrete case, it analogously holds that</p>
<p>$0 \lt H(\textbf{p}) \le \log(n)$</p>
<p>with the maximum $\log(n)$ at $\textbf{p} = [\frac{1}{n}, \dots, \frac{1}{n}]$ and the minimum $0$ at
probabilities concentrated in one spot (a so called one-hot probability).</p>
<p>The entropy $H$ of a continuous probability density $f$ is defined as:</p>
<p>$H(f) = \int_{\operatorname{supp} (f)} f(t)\log(f(t))dt$,</p>
<p>The entropy of the uniform distribution on $[0, T]$ is simply $\log(T)$.
Note that in the continuous case, the entropy can be $0$ (for $T = 1$) or negative
(for $T \in (0, 1)$).
The entropy of the normal distribution $\mathcal{N}(\mu, \sigma)$ is $\frac {1}{2}(1+\log(2\sigma ^{2}\pi))$,
independent of $\mu$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Entropy-(Pytorch)">Entropy (Pytorch)<a class="anchor-link" href="#Entropy-(Pytorch)"> </a></h3><p>Pytorch feature numerous distributions; they all come with their entropy function.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torch.distributions.bernoulli</span> <span class="kn">import</span> <span class="n">Bernoulli</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">arange</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([</span><span class="n">Bernoulli</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="o">.</span><span class="n">entropy</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">x</span><span class="p">])</span>
<span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;Bernoulli&#39;</span> <span class="p">,</span> <span class="s1">&#39;p&#39;</span><span class="p">,</span> <span class="s1">&#39;entropy&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
" />
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torch.distributions.normal</span> <span class="kn">import</span> <span class="n">Normal</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">arange</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([</span><span class="n">Normal</span><span class="p">(</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">]),</span> <span class="n">tensor</span><span class="p">([</span><span class="n">s</span><span class="p">]))</span><span class="o">.</span><span class="n">entropy</span><span class="p">()</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">x</span><span class="p">])</span>
<span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;Normal&#39;</span> <span class="p">,</span> <span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="s1">&#39;entropy&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
" />
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Some-useful-functions">Some useful functions<a class="anchor-link" href="#Some-useful-functions"> </a></h3><p>We introduce $\operatorname{odds}$, $\operatorname{logit}$ and $\operatorname{softmax}$.</p>
<p>"The odds are 5" is a way of saying that winning is 5 times more likely than losing,
which means that $p/(1 - p) = 5$, or $p = 5/6$. The definition is:</p>
<p>$\operatorname{odds}(p) = \frac{p}{1-p}$.</p>
<p>$\operatorname{logit}$ is the $\log$ of odds:</p>
<p>$\operatorname{logit}(p) = \log(\frac{p}{1-p})$,</p>
<p>and $\operatorname{sigmoid}$ is the inverse of $\operatorname{logit}$:</p>
<p>$\operatorname{sigmoid}(x) = \frac{1}{1 + \exp(-x)}$,</p>
<p>which is thus confirmed:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">(</span><span class="mf">2.</span><span class="p">)</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># logit undoes sigmoid</span>
<span class="n">show_diff</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">logit</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="n">x</span><span class="p">)</span>

<span class="c1"># sigmoid undoes logit</span>
<span class="n">show_diff</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">logit</span><span class="p">(</span><span class="n">p</span><span class="p">)),</span> <span class="n">p</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>-4.76837158203125e-07
0.0
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>$\operatorname{sigmoid}$ normalizes any real number to the open interval $(0, 1)$.
It is frequently used as activation function in neuronal networks.</p>
<p>$\operatorname{softmax}$ does for vectors what $\operatorname{sigmoid}$ does for scalars. It normalizes any
vector $\textbf{x} = [x_1, \dots, x_n]$ of real numbers (e.g. the output
of a neuronal network) to a probability distribution:</p>
<p>$\operatorname{softmax}(\textbf{x}) = \left[ \frac{\exp(x_i)}{\sum_{j=1}^{n}exp(x_j)} \right]_{i=1,\dots, n}$</p>
<p>$\operatorname{softmax}$ is often combined with $\log$ for better numerical stability.
$\operatorname{softmax}$ is only one of many possible normalizations. So, for instance, it would be natural to
normalize $[\operatorname{logit}(p), \operatorname{logit}(1-p)]$ to $[p, 1-p]$, but here is
what $\operatorname{softmax}$ does:</p>
<p>$\operatorname{softmax}([\operatorname{logit}(p), \operatorname{logit}(1-p)]) =
\left[\frac{p^2}{p^2+(1-p)^2}, \frac{(1-p)^2}{p^2+(1-p)^2}\right]$,</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Cross-Entropy-(Recap)">Cross Entropy (Recap)<a class="anchor-link" href="#Cross-Entropy-(Recap)"> </a></h3><p>The cross entropy of two discrete probability distributions $\textbf{p}$ and $\textbf{q}$ is defined as</p>
<p>$H(\textbf{p}, \textbf{q}) = -E_p [\log(\textbf{q})] = -\sum_{i=1}^{n-1} p_i \log(q_i)$</p>
<p>The cross entropy tells us how much
information is contained in $\textbf{q}$ given $\textbf{p}$.
It can be thought of as the gap between $\textbf{p}$ and $\textbf{q}$:
the more additional information $\textbf{q}$ carries, the larger the gap. In more precise terms:</p>
<blockquote><p>$H(p, q)$ is the expected extra message-length per datum that must be communicated if a code
that is optimal for a given (wrong) distribution $q$ is used, compared to using a code based
on the true distribution $p$:it is the excess entropy(see <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Motivation">Wikipedia</a> for details).</p>
</blockquote>
<p>It is not symmetric, and we have $H(\textbf{p}, \textbf{p}) = H(\textbf{p})$ which is never zero in the discrete case.</p>
<p>The business case of cross entropy is as follows:
A known distribution $\textbf{p}$ (the one before the $\log$) is considered the truth or best guess.
A second distribution $\textbf{q}$ (the one inside the $\log$)
is assumed to be Bernoulli, Normal or whatever. The problem is to minimize
the gap $H(\textbf{p}, \textbf{q})$ between $\textbf{p}$ and $\textbf{q}$ by
choosing optimal parameters of the assumed distribution.
In neuronal networks, cross entropy is used exactly as any other distance
such as the mean square error (MSE).</p>
<h3 id="Cross-Entropy-(Pytorch)">Cross Entropy (Pytorch)<a class="anchor-link" href="#Cross-Entropy-(Pytorch)"> </a></h3><p>There are two Pytorch functions which do the job: <code>CrossEntropyLoss</code> and <code>BCELoss</code>
(binary cross entropy loss) for the binary case.</p>
<p>Let us look at <code>CrossEntropyLoss</code> first. Up to some details, it
takes two arrays of distributions (two matrices with shape <code>(m, K)</code>)
and computes the mean cross entropy of all pairs.
To understand the details, we proceed in three steps:</p>
<ol>
<li><p>We define a function <code>CELoss1(Q: Tensor, P: Tensor) -&gt; Tensor</code> which accepts two matrices <code>Q</code> and <code>P</code>,
each row being a probability distribution. The result is the mean of cross entropies <code>H(Q(i), P(i))</code>
(a tensor with one float element).
Note that the order of arguments has changed with respect to $H(p, q)$:
now the reference distribution <code>P</code> comes last.
The shape of both <code>P</code> and <code>Q</code> is <code>(m, K)</code>:
<code>m</code> is the number of observations (or batch size),
<code>K</code> the number of different classes; in the Bernoulli case, we have <code>K = 2</code>.</p>
</li>
<li><p>We define a function <code>CELoss2(Q: Tensor, y: Tensor) -&gt; Tensor</code>, in which the second argument <code>P</code> is
replaced with a one-hot representation: all but one <code>p[i]</code> are zero, and exactly one, say <code>p[k]</code> equals one,
which is expressed by <code>y[i] = k</code>. So, the vector <code>y</code> replaces a huge spare matrix with exactly one non-zero
element in each row (this is called the incidence matrix).</p>
</li>
<li><p>We define a function <code>CELoss3(X: Tensor, y: Tensor) -&gt; Tensor</code> which differs from <code>cross_entropy_onehot</code>
in that each row of the first argument <code>X</code> is transformed into a probability distribution via <code>softmax</code>.
So, <code>CELoss3</code> accepts any K-dimensional input. Up to some additional parameters does
this function the same as Pytorch's <code>CELoss</code>.</p>
</li>
</ol>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">CELoss1</span><span class="p">(</span><span class="n">Q</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">P</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    @param Q: matrix of estimated probability densities to be compared to P, shape = (m, K)</span>
<span class="sd">    @param P: matrix of reference probability densities, shape = (m, K)</span>
<span class="sd">    @return: -mean of row wise cross entropy of Q with respect to P</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># log(Q) has shape (m, K); log is elementwise applied</span>
    <span class="c1"># log(Q) * P has shape (m, K); this is the elementwise product</span>
    <span class="c1"># torch.sum(torch.log(Q) * P, 1) yields the vector of row sums</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span> <span class="o">*</span> <span class="n">P</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">CELoss2</span><span class="p">(</span><span class="n">Q</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    @param Q: matrix of estimated probability densities to be compared to y, shape = (m, K)</span>
<span class="sd">    @param y: vector of true labels; 0 &lt;= y[i] &lt; K = Q.shape[1], shape = (m,)</span>
<span class="sd">    y represents a one-hot probability</span>
<span class="sd">    @return: mean of row wise cross entropy of Q with respect to p</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">Q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># m</span>
    <span class="c1"># Q[x, y] is the same as [Q[i, y[i]] for i in range(m)]</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">]))</span>

<span class="k">def</span> <span class="nf">CELoss3</span><span class="p">(</span><span class="n">X</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    @param X: any vector of shape (n,)</span>
<span class="sd">    X is transformed into a probability density Q via softmax</span>
<span class="sd">    @param y: the true label; 0 &lt;= y &lt; K = q.shape</span>
<span class="sd">    y represents the one-hot probability</span>
<span class="sd">    @return: cross entropy of q with respect to y</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">CELoss2</span><span class="p">(</span><span class="n">softmax</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>

<span class="n">CELoss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

<span class="c1"># CELoss and CELoss3 yield almost the same results.</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">],</span>
            <span class="p">[</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">7.</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>

<span class="n">show_diff</span><span class="p">(</span><span class="n">CELoss</span><span class="p">(</span><span class="n">X</span><span class="p">[[</span><span class="mi">1</span><span class="p">]],</span> <span class="n">y</span><span class="p">[[</span><span class="mi">1</span><span class="p">]]),</span> <span class="n">CELoss3</span><span class="p">(</span><span class="n">X</span><span class="p">[[</span><span class="mi">1</span><span class="p">]],</span> <span class="n">y</span><span class="p">[[</span><span class="mi">1</span><span class="p">]]))</span>
<span class="n">show_diff</span><span class="p">(</span><span class="n">CELoss</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">CELoss3</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>-2.9802322387695312e-08
-2.9802322387695312e-08
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="A-Numerical-Issue">A Numerical Issue<a class="anchor-link" href="#A-Numerical-Issue"> </a></h3><p>In <code>CELoss3</code> we first compute <code>softmax</code> with lots of exponents and then take the <code>log</code> in <code>CELoss2</code>.
It is convenient to shift the <code>log</code> from <code>CELoss2</code> to <code>CELoss3</code>.
So, <code>CELoss2</code> becomes <code>NLLLoss1</code>, equivalent to <code>NLLLoss</code>, and
<code>CELoss3</code> becomes <code>CELoss4</code>, equivalent to <code>CrossEntropyLoss</code>.
<code>NLLLoss</code> is so called because it expects its input to be just that, a non-negative log loss.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">NLLLoss1</span><span class="p">(</span><span class="n">Q</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Negative Loss Likelihood Loss</span>
<span class="sd">    @param Q: matrix of estimated probability densities to be compared to y, shape = (m, K)</span>
<span class="sd">    @param y: the true label; 0 &lt;= y &lt; K = q.shape</span>
<span class="sd">    @return: -mean of [Q[i, y[i]] for i in range(m)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">Q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># m</span>
    <span class="c1"># Q[x, y] is the same as [Q[i, y[i]] for i in range(m)]</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">CELoss4</span><span class="p">(</span><span class="n">X</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This is identical to CELoss3 but numerically more stable</span>
<span class="sd">    @param X: any vector of shape (n,)</span>
<span class="sd">    X is transformed into a probability density Q via softmax</span>
<span class="sd">    @param y: the true label; 0 &lt;= y &lt; K = q.shape</span>
<span class="sd">    This represents the one-hot probability p concentrated at y</span>
<span class="sd">    @return: cross entropy of q with respect to y</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">NLLLoss1</span><span class="p">(</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>


<span class="c1"># CELoss and CELoss4 yield exactly the same results</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">],</span>
            <span class="p">[</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">7.</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>

<span class="n">show_diff</span><span class="p">(</span><span class="n">CELoss</span><span class="p">(</span><span class="n">X</span><span class="p">[[</span><span class="mi">1</span><span class="p">]],</span> <span class="n">y</span><span class="p">[[</span><span class="mi">1</span><span class="p">]]),</span> <span class="n">CELoss4</span><span class="p">(</span><span class="n">X</span><span class="p">[[</span><span class="mi">1</span><span class="p">]],</span> <span class="n">y</span><span class="p">[[</span><span class="mi">1</span><span class="p">]]))</span>
<span class="n">show_diff</span><span class="p">(</span><span class="n">CELoss</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">CELoss4</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>0.0
0.0
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Cross-Entropy-Loss-and-Binary-Cross-Entropy-(Pytorch)">Cross Entropy Loss and Binary Cross Entropy (Pytorch)<a class="anchor-link" href="#Cross-Entropy-Loss-and-Binary-Cross-Entropy-(Pytorch)"> </a></h3><p>As the rows of <code>P</code>, the first argument of <code>CELoss</code>, sum up to 1, we could dispose of one column.
This is what <code>BCELoss</code> does in the binary case. It serves as a shorthand for <code>CrossEntropyLoss</code>.
Its input are two tensors of shape <code>(m,)</code>, the first one, <code>P</code>, with probabilities,
the second one, <code>y</code> with binary labels. The missing second column of <code>P</code> is computed as <code>1 - P</code>.
See <code>BCELoss1</code> for an implementation.</p>
<p><code>BCELoss</code> is not quite consistent with <code>CrossEntropyLoss</code>.
Comparing <code>BCELoss</code> with <code>CELoss</code>is tricky because the latter applies
<code>softmax</code> while the former uses <code>sigmoid</code>. We proceed as follows:</p>
<p>The input to <code>CELoss</code> is some tensor <code>X</code> and binary labels <code>y</code> both with shape <code>(m, 2)</code>.
The input to <code>BCELoss</code> is obtained by taking the second column of <code>softmax(X)</code>.
<code>BCELoss</code> requires the binary labels to be <code>float</code>.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">BCELoss1</span><span class="p">(</span><span class="n">q</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="n">ones</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">q</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">ones</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">ones</span> <span class="o">-</span> <span class="n">q</span><span class="p">))</span>

<span class="n">CELoss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">BCELoss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()</span>

<span class="c1"># Input for CELoss:</span>
<span class="c1"># X.shape = (m, 2), because it&#39;s binary</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([[</span><span class="mf">8.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">],</span> <span class="p">[</span><span class="mf">8.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">]])</span>
<span class="c1"># y contains only 0 and 1</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>

<span class="c1"># Input for BCELoss (and BCELoss1):</span>
<span class="c1"># softmax(X, dim=1) contains in each row a binary probability density</span>
<span class="c1"># Q is the second column of softmax(X, dim=1)</span>
<span class="n">Q</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">y1</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>  <span class="c1"># doesn&#39;t make sense</span>

<span class="c1"># CELoss and BCELoss are almost equal,</span>
<span class="n">show_diff</span><span class="p">(</span><span class="n">CELoss</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">BCELoss</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">y1</span><span class="p">))</span>

<span class="c1"># while BCELoss and BCELoss1 completely agree.</span>
<span class="n">show_diff</span><span class="p">(</span><span class="n">BCELoss</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">y1</span><span class="p">),</span> <span class="n">BCELoss1</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>-7.450580596923828e-09
0.0
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="BCELoss-and-BCEWithLogitsLoss-(Pytorch)">BCELoss and BCEWithLogitsLoss (Pytorch)<a class="anchor-link" href="#BCELoss-and-BCEWithLogitsLoss-(Pytorch)"> </a></h3><p><code>BCELoss</code> comes in two varieties: The first one, <code>BCELoss</code> accepts as first
argument a vector <code>P</code> of probabilities.
This corresponds to <code>CELoss1</code>. The second one, <code>BCEWithLogitsLoss</code>, accepts as first argument any real vector
which is normalized by <code>sigmoid</code> rather than <code>softmax</code> as in <code>CrossEntropyLoss</code>.
<code>BCEWithLogitsLoss</code> is so called because the first argument is expected to be a logit.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">BCEWithLogitsLoss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">()</span>
<span class="n">logit</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">logit</span>

<span class="n">Q</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>

<span class="n">show_diff</span><span class="p">(</span><span class="n">BCELoss</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">BCEWithLogitsLoss</span><span class="p">(</span><span class="n">logit</span><span class="p">(</span><span class="n">Q</span><span class="p">),</span> <span class="n">y</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>0.0
</pre>
</div>
</div>

</div>
</div>

</div>
    

</div>



  </div><a class="u-url" href="/testfast/2021/01/02/crossentropy.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/testfast/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/testfast/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/testfast/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An easy to use blogging platform with support for Jupyter Notebooks.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/johsieders" target="_blank" title="johsieders"><svg class="svg-icon grey"><use xlink:href="/testfast/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
